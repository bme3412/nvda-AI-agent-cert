// Article summaries for display above navigation buttons
// Each summary is 2 sentences describing the article's key content

export const ARTICLE_SUMMARIES: Record<string, string> = {
  '1-agent-architecture-design/1-agentic-AI-factory': 'An Enterprise AI Factory is essentially a sophisticated, industrialized platform for building and deploying AI agents at scale. Think of it as a modern assembly line, but instead of manufacturing physical products, it orchestrates the entire lifecycle of intelligent software agents that can reason, plan, and execute complex tasks autonomously.',
  
  '1-agent-architecture-design/2-building-autonomous-AI': 'Traditional large language models represent a powerful but fundamentally limited paradigm—they excel at generating text responses but lack the ability to autonomously plan multi-step workflows, take actions in the real world, or maintain adaptive memory across interactions. Agentic AI represents a transformative shift from these static, single-turn systems to autonomous agents that can perceive goals, generate structured plans, execute actions by interfacing with external tools, and continuously adapt based on environmental feedback.',
  
  '1-agent-architecture-design/3-building-blocks-customer-service': 'In today\'s business landscape, exceptional customer service isn\'t optional—it\'s fundamental to competitive survival. The NVIDIA AI Blueprint for AI virtual assistants provides a production-ready framework for scaling customer service operations with generative AI while maintaining data integrity and governance.',
  
  '1-agent-architecture-design/4-agentic-AI-autonomous-AI-agents': 'Traditional large language models operate in isolation, responding to single prompts without the capacity for structured planning, autonomous action, or adaptive memory across interactions. Agentic AI represents a fundamental evolution beyond this limitation—systems that can autonomously perceive goals, generate multi-step plans, execute actions by interfacing with external tools, adapt based on environmental feedback, and reason over time through continuous cycles of goal identification, planning, action execution, memory updates, and feedback reflection.',
  
  '1-agent-architecture-design/5-catch-me-if-you-can': 'Modern financial fraud detection systems face a fundamental paradox: they\'re built on models trained from historical patterns, yet fraud evolves faster than retraining cycles, with adversaries constantly adapting through bots, social engineering, synthetic identities, and adversarial behaviors that render yesterday\'s detection rules obsolete. Agentic systems composed of specialized, collaborative AI agents offer a paradigm shift by breaking fraud detection into interpretable, adaptable components rather than relying on one monolithic model or rigid pipeline.',
  
  '1-agent-architecture-design/6-multi-agent-systems': 'Multi-agent systems—teams of specialized AI agents working collaboratively—represent the next evolution beyond single-agent AI, introducing capabilities like planning, reasoning, contextual memory, and autonomous tool use to facilitate complex workflows with minimal human intervention. Rather than relying on one monolithic AI to handle everything, these systems distribute intricate tasks across multiple agents, each with specific roles, specialized capabilities, autonomous operation, and a local view of the system.',
  
  '1-agent-architecture-design/7-designing-user-interfaces': 'When you\'re building an AI agent, the interface isn\'t just a nice-to-have feature—it\'s the entire bridge between human intent and machine action. The foundation of intuitive human-agent interaction starts with transparency and visibility, where users need to see what the agent is thinking and doing, not just the final output.',
  
  '1-agent-architecture-design/8-implementing-reasoning-react': 'ReAct bridges the gap between reasoning and action by creating a continuous loop where agents alternate between reasoning about what they should do next and actually taking actions based on that reasoning. The beauty of ReAct is in its explicit reasoning traces, making the agent\'s behavior interpretable and debuggable while allowing course-correction mid-execution.',
  
  '1-agent-architecture-design/9-configuring-agent-communication': 'Most interesting problems are too complex for a single agent to solve effectively, which is where multi-agent systems come in, and with them, the critical challenge of getting these agents to actually talk to each other in productive ways. The foundation of any multi-agent communication system is the message structure, where agents need a common language—not just natural language, but a standardized way of packaging information.',
  
  '1-agent-architecture-design/10-managing-memory': 'Here\'s the fundamental problem with AI agents: they\'re incredibly intelligent in the moment but naturally have the memory span of a goldfish. Without deliberate memory architecture, an agent might brilliantly solve a complex problem for you on Monday and then have absolutely no recollection of it by Tuesday.',
  
  '1-agent-architecture-design/11-orchestrating-multi-agent-workflows': 'Multi-agent orchestration is about creating a system where specialized agents collaborate seamlessly to accomplish complex tasks that no single agent could handle alone. The fundamental question is who\'s in charge—centralized orchestration employs a single supervisor agent coordinating tasks, while distributed orchestration allows agents to coordinate among themselves through direct communication and negotiation.',
  
  '1-agent-architecture-design/12-applying-logic-trees': 'Logic trees give structure to how agents break down complex problems into manageable decision points, where each node represents a decision or question, and branches represent different possible answers or paths forward. The power of logic trees is that they make reasoning explicit and traceable, enabling pruning strategies that make complex reasoning computationally feasible.',
  
  '1-agent-architecture-design/13-implementing-knowledge-graphs': 'A knowledge graph is fundamentally a way of representing information that preserves and makes explicit the relationships between pieces of knowledge, structuring information as a network of entities connected by labeled relationships. When agents integrate with knowledge graphs, they gain multi-hop reasoning capabilities that are nearly impossible with pure text-based knowledge, enabling systematic traversal of relationship chains.',
  
  '1-agent-architecture-design/14-ensuring-adaptability-scalability': 'The difference between a successful agentic AI deployment and an expensive failure often comes down to whether you built for adaptability and scalability from the beginning or treated them as problems you\'d solve "later." Scalability isn\'t just about handling more users—it\'s about your system\'s ability to handle increases across multiple dimensions simultaneously without falling apart or requiring complete architectural redesign.',

  // Section 2: Agent Development
  '2-agent-development/1-optimization-NVDA-Triton': 'Think of Triton Inference Server as a restaurant kitchen trying to serve customers as efficiently as possible. The single biggest performance win comes from dynamic batching, which intelligently groups inference requests into larger batches that execute far more efficiently on GPUs, delivering nearly 4x improvement in throughput.',
  
  '2-agent-development/2-NVDA-agent-intelligence-toolkit': 'Think of NVIDIA\'s Agent Intelligence (AIQ) Toolkit as a universal adapter and monitoring system for AI agents—it\'s designed to work alongside whatever agent framework you\'re already using, whether that\'s LangChain, LlamaIndex, CrewAI, or even your own custom Python agents. The framework-agnostic design is what makes AIQ special, standardizing profiling tools, evaluation metrics, and debugging interfaces across all platforms.',
  
  '2-agent-development/3-intro-LLM-p-tuning': 'At their core, language models do something deceptively simple: they predict which word should come next in a sequence. What changed with Large Language Models (LLMs) is scale, and scale brings surprising new capabilities where models with a billion or more parameters can suddenly perform tasks like summarization, translation, content generation, and reasoning that smaller models struggle with.',
  
  '2-agent-development/4-building-multi-modal-AI-RAG': 'Imagine you have thousands of documents—PDFs full of text and images, PowerPoint presentations with charts and graphs, standalone images with diagrams. Multimodal RAG solves this by extending the pipeline to handle various data types, converting everything into a single modality—text—before proceeding with the traditional RAG workflow.',
  
  '2-agent-development/5-design-considerations-Agentic': 'Imagine you\'re managing a team where everyone has specialized skills—one person extracts data, another integrates it, someone else handles databases, and a manager coordinates everything. Agentic AI works the same way: instead of one monolithic system doing everything, you build modular components (agents) that each handle specific tasks, working collaboratively under a coordinator (the Global Agent).',
  
  '2-agent-development/6-transient-fault-handling': 'Imagine you\'re trying to call a friend and the line is busy, or the connection drops mid-sentence, or there\'s static that clears up after a moment. Transient faults in cloud computing work the same way: temporary glitches that resolve themselves if you just retry after a suitable delay.',
  
  '2-agent-development/7-circuit-breaker-pattern': 'Imagine you\'re trying to reach a friend by phone, but their line is busy. The Circuit Breaker pattern prevents applications from repeatedly attempting operations that are likely to fail, allowing systems to fail fast, conserve resources, and give struggling services space to recover.',
  
  '2-agent-development/8-retry-pattern': 'Imagine calling a busy restaurant to make a reservation. The Retry pattern does for cloud applications what waiting and calling back does for busy phone lines: when a request fails, it waits a bit and tries again, banking on the failure being temporary rather than permanent.',
  
  '2-agent-development/9-evaluating-decision-strategies': 'Evaluating and refining agent decision-making strategies represents the critical process of systematically assessing how well your AI agents make choices, identifying weaknesses in their reasoning and actions, and iteratively improving their performance over time. Effective evaluation and refinement requires combining quantitative metrics with qualitative assessment, automated testing with human judgment, controlled experiments with production monitoring, and short-term fixes with long-term systematic improvements.',

  // Section 3: Evaluation and Tuning
  '3-evaluation-tuning/1-powering-next-gen-AI-agents': 'Agentic AI represents a fundamental shift in artificial intelligence capabilities, moving beyond simple prompt-and-response interactions to sophisticated systems that use advanced reasoning and planning to solve complex, multi-step problems autonomously. The transformative power of agentic AI lies in its ability to learn and improve continuously through what\'s known as a data flywheel, where human and AI feedback is systematically used to refine models and improve outcomes.',
  
  '3-evaluation-tuning/2-agent-intelligence-toolkit-overview': 'The NVIDIA NeMo Agent Toolkit represents a paradigm shift in agent development tooling, providing framework-agnostic observability and evaluation that works alongside any agent framework without requiring rebuilds or framework lock-in. The toolkit\'s core philosophy emphasizes reusability and composability, with every agent, tool, and workflow existing as a function call that works together in complex software applications.',
  
  '3-evaluation-tuning/3-agent-intelligence-toolkit-tutorials': 'Workflows form the heart of the NeMo Agent Toolkit, defining which agentic tools and models are used to perform a given task or series of tasks. Workflow configuration files serve as the blueprint for agentic systems, specifying tools, models, and behavior through structured YAML files.',
  
  '3-evaluation-tuning/4-agent-intelligence-toolkit-FAQ': 'The NVIDIA Agent Intelligence (AIQ) toolkit is a flexible, opt-in system designed to enhance your existing agentic workflows without requiring major code rewrites. It\'s not a new LLM or agentic framework attempting to replace what you\'re already using, but rather a universal integration layer that works alongside existing frameworks.',
  
  '3-evaluation-tuning/5-launching-agent-intelligence-toolkit': 'The NVIDIA Agent Intelligence (AIQ) toolkit provides a user interface that allows you to interact with your running workflows in several ways. Starting the AIQ toolkit server is straightforward using the aiq serve command along with a configuration file that defines your workflow.',
  
  '3-evaluation-tuning/6-NVDA-NEMO-agent': 'The NVIDIA NeMo Agent Toolkit is a flexible, lightweight library designed to help you connect existing enterprise agents to data sources and tools across any framework. The toolkit has recently added several powerful new features including automatic hyperparameter tuning, support for Google\'s Agent Development Kit (ADK) framework, and Function Groups that allow you to package multiple related functions together.',
  
  '3-evaluation-tuning/7-agentic-AI-next-big-thing': 'Agentic AI represents a fundamental shift in artificial intelligence, moving from passive assistance to active, autonomous decision-making. Unlike traditional AI systems that operate on a simple input/output model requiring constant human supervision, agentic AI can make autonomous decisions based on both past performance and current assessments of what\'s needed to accomplish a task.',
  
  '3-evaluation-tuning/8-agentic-AI-challenges': 'Agentic AI represents the next evolution beyond generative AI, moving from simple one-step prompt-and-response interactions to complex multi-step processes where AI systems interact with different platforms to achieve desired outcomes. The first major challenge is model logic and critical thinking, where the critical thinker model needs training on data that\'s closely grounded in reality, with extensive information on specific goals, plans, actions, and results.',
  
  '3-evaluation-tuning/9-AI-agents-beginners': 'As AI agents transition from experimental prototypes to real-world applications, the ability to understand their behavior, monitor their performance, and systematically evaluate their outputs becomes critically important. Observability tools typically represent agent runs using two key concepts: traces and spans, where a trace represents a complete agent task from start to finish, while spans are the individual steps within that trace.',
  
  '3-evaluation-tuning/10-navigating-challenges': 'As AI evolves into autonomous decision-making systems known as agentic AI, organizations and industries are being reshaped in fundamental ways. The first critical pitfall is taking a technology-only approach, where companies focus solely on technology while ignoring the broader organizational context, but successful AI transformation requires a holistic approach encompassing strategy, capabilities, ethical standards, and workforce development.',

  // Section 4: Deployment and Scaling
  '4-deployment-scaling/1-agentic-AI-factory': 'The Enterprise AI Factory represents a comprehensive, cloud-native platform built around Kubernetes that provides agility, scalability, and resilience for developing and deploying sophisticated AI agents. Storage infrastructure forms a critical foundation that must be architected correctly to avoid becoming a bottleneck in the AI development and deployment lifecycle.',
  
  '4-deployment-scaling/2-TensorRTLLM-Github': 'TensorRT LLM is an open-sourced library specifically designed for optimizing Large Language Model inference performance on NVIDIA GPUs. The library incorporates comprehensive optimization techniques designed to maximize inference performance, including custom attention kernels, inflight batching capabilities, paged KV caching, and extensive quantization support.',
  
  '4-deployment-scaling/3-measure-improve-AI-workload': 'NVIDIA DGX Cloud Benchmarking provides organizations with comprehensive tools to assess real-world, end-to-end AI workload performance and total cost of ownership, moving beyond simple comparisons of raw FLOPs or hourly GPU costs. GPU scaling optimization reveals significant opportunities for reducing training time without proportionally increasing costs, with extensive testing demonstrating that scaling GPU count in AI training clusters can dramatically reduce total training time while maintaining cost efficiency.',
  
  '4-deployment-scaling/4-Kubernetes-glossary': 'Kubernetes is an open-source platform for automating container orchestration—the deployment, scaling, and management of containerized applications. Container technology gained popularity with Docker\'s introduction in 2013, offering significant advantages over virtual machines by virtualizing the operating system instead of hardware.',
  
  '4-deployment-scaling/5-NVDA-NSight': 'NVIDIA Nsight Systems is a comprehensive system-wide performance analysis tool designed to visualize application algorithms, identify optimization opportunities, and tune performance to scale efficiently across any quantity or size of CPUs and GPUs. CPU-GPU interaction visualization represents a core strength of Nsight Systems, as the tool latches onto target applications to expose GPU and CPU activity, events, annotations, throughput, and performance metrics in chronological timelines with minimal overhead.',
  
  '4-deployment-scaling/6-Kube-Prometheus': 'Setting up a comprehensive monitoring solution for GPU-accelerated AI workloads requires implementing a Prometheus stack that can be efficiently managed through the Helm package manager and the Prometheus Operator with kube-prometheus projects. NVIDIA Data Center GPU Manager (DCGM) Exporter deployment provides essential GPU telemetry collection capabilities through a dedicated Helm chart that integrates seamlessly with the Prometheus monitoring stack.',
  
  '4-deployment-scaling/7-scaling-LLM-Kubernetes': 'Large language models have become essential for chatbots, content generation, summarization, classification, and translation applications, with state-of-the-art foundation models like Llama, Gemma, GPT, and Nemotron demonstrating human-like understanding and generative capabilities. Kubernetes enables dynamic scaling of LLM deployments from single GPU configurations to multi-GPU clusters capable of handling thousands of real-time inference requests with low latency and high accuracy.',
  
  '4-deployment-scaling/8-TensorRT-performance-analysis': 'NVIDIA Nsight Systems provides comprehensive application-level performance analysis capabilities that prove essential for optimizing Large Language Model inference workloads. The core profiling functionality centers on dynamic control of profiling data collection through runtime API toggling mechanisms, allowing users to specify exactly which regions correspond to profiled sections.',

  // Section 5: Cognition, Planning, Memory
  '5-cognition-planning-memory/1-NVDA-NeMO': 'NVIDIA NeMo™ is a modular software suite for managing the AI agent lifecycle, providing microservices and toolkits for data processing, model fine-tuning and evaluation, reinforcement learning, policy enforcement, and system observability. The platform provides modular AI agent lifecycle management, enabling organizations to manage the complete lifecycle—from data curation, customization, and evaluation to guardrailing, observability, and optimization—with an enterprise-ready, interoperable software suite.',
  
  '5-cognition-planning-memory/2-LLM-context-learners': 'Large language models have revolutionized AI capabilities, but they face a fundamental limitation: context windows that constrain how much information they can process in a single interaction. Context learners represent advanced techniques that enable LLMs to effectively manage and utilize extended context windows, allowing them to process longer documents, maintain coherent multi-turn conversations, and access more comprehensive information during reasoning.',
  
  '5-cognition-planning-memory/3-NemO-RL-doc': 'NeMo RL is an open-source post-training library within the NeMo Framework, designed to streamline and scale reinforcement learning methods for multimodal models, including LLMs and VLMs. The library offers flexibility with a modular design that allows easy integration and customization, enabling researchers and developers to adapt the framework to their specific needs.',
  
  '5-cognition-planning-memory/4-Jamba-1.5': 'AI21 Labs has unveiled their latest and most advanced Jamba 1.5 model family, a cutting-edge collection of large language models (LLMs) designed to excel in a wide array of generative AI tasks. The Jamba 1.5 model family is built with a unique hybrid approach that combines the strengths of the Mamba and transformer architectures, in addition to a mixture of experts (MoE) module.',
  
  '5-cognition-planning-memory/5-Understanding-planning-LLM': 'LLM Agent Planning represents the application of Large Language Models as the cognitive core of autonomous agents capable of perceiving environments, generating action sequences, and accomplishing specific tasks. LLM-based agent planning offers several transformative advantages over conventional planning methods, providing natural language understanding capabilities that enable agents to interpret flexible, human-described problems without requiring conversion into symbolic representations.',
  
  '5-cognition-planning-memory/6-AI-Agent-Memory': 'AI Memory represents the ability of Large Language Model-driven systems to encode, store, retain, and retrieve information from past interactions to improve future responses and decision-making. Memory serves as the foundation for creating personalized, continuous, and context-aware AI experiences that can adapt to user needs and evolving task requirements.',
  
  '5-cognition-planning-memory/7-Understanding-Planning-LLM': 'LLM Agent Planning represents the application of Large Language Models as the cognitive core of autonomous agents capable of perceiving environments, generating action sequences, and accomplishing specific tasks. Planning, as a critical capability for agents, requires sophisticated understanding, reasoning, and decision-making processes.',

  // Section 6: Knowledge Integration and Data Handling
  '6-knowledge-integration-data-handling/1-RAG-more-accurate': 'Retrieval Augmented Generation and Fine-Tuning represent two fundamental approaches for enhancing Large Language Model capabilities and adapting them to specific tasks, domains, and organizational requirements. Retrieval Augmented Generation extends language models during the inference phase by providing access to external knowledge sources, enabling retrieval of information not stored in model parameters.',

  // Section 7: NVIDIA Platform Integration
  '7-NVDA-Platform-Integration/1-Best-Practices-TensorRT': 'TensorRT performance optimization encompasses systematic approaches to measuring, analyzing, and enhancing deep learning inference execution on NVIDIA GPU architectures. Performance optimization begins with rigorous measurement using specialized benchmarking tools that quantify inference latency, throughput characteristics, per-layer execution times, and resource consumption patterns.',
  
  '7-NVDA-Platform-Integration/2-Batchers': 'Triton Inference Server batching represents dynamic request aggregation mechanisms that combine multiple inference requests into unified batches for improved computational efficiency and throughput optimization. Batching operates transparently to client applications, with Triton\'s scheduling infrastructure managing request aggregation, batch formation, instance distribution, and response coordination automatically.',
  
  '7-NVDA-Platform-Integration/3-Triton-server-backend': 'Triton Inference Server backends represent the execution implementations that process model inference requests, serving as the interface between Triton\'s orchestration infrastructure and actual model computation logic. Each model deployed in Triton associates with exactly one backend that handles inference execution for that model, with backend selection specified through model configuration parameters.',
  
  '7-NVDA-Platform-Integration/4-NeMO-Guardrails': 'NVIDIA NeMo Guardrails represents a scalable orchestration framework for implementing safety, security, and compliance controls in AI applications powered by large language models and autonomous agents. The framework enables definition, orchestration, and enforcement of programmable guardrails addressing content safety, topic control, personally identifiable information detection, retrieval-augmented generation grounding, and jailbreak prevention.',
  
  '7-NVDA-Platform-Integration/5-performance-tuning-guide': 'NeMo Framework performance optimization encompasses comprehensive strategies and techniques for maximizing training efficiency, memory utilization, and computational throughput when training large language models on GPU infrastructure. Performance optimization requires systematic analysis of factors affecting training efficiency including model architecture characteristics, hyperparameter selections, GPU counts and types, and workload characteristics.',
  
  '7-NVDA-Platform-Integration/6-NeMo-best-practices': 'NeMo Curator memory management encompasses strategies and techniques for efficiently processing large-scale text datasets within constrained memory resources during data curation workflows. The memory management framework provides comprehensive controls over data partitioning, batch processing, and operation-specific optimizations that collectively enable processing of massive text corpora on finite hardware resources.',
  
  '7-NVDA-Platform-Integration/7-Optimization': 'Triton Inference Server optimization encompasses systematic strategies and techniques for maximizing inference throughput, minimizing latency, and improving resource utilization when deploying models in production serving environments. Performance optimization requires careful analysis of latency-throughput tradeoffs where improvements in one dimension often involve compromises in the other.',
  
  '7-NVDA-Platform-Integration/8-NEmo-Agent': 'NVIDIA NeMo Agent Toolkit represents an open-source framework for building, profiling, and optimizing AI agent systems across diverse agent frameworks, enabling unified cross-framework integration and comprehensive observability for enterprise-scale agentic deployments. The framework operates as universal integration layer supporting major agent frameworks including LangChain, CrewAI, and custom implementations through standardized interfaces.',
  
  '7-NVDA-Platform-Integration/9-Nemo-Agent-intelligence-toolkit': 'NVIDIA NeMo Agent Toolkit represents a flexible lightweight framework for connecting enterprise agents to data sources and tools across heterogeneous agent framework ecosystems. The integration architecture operates alongside and around existing agentic frameworks including LangChain, LlamaIndex, CrewAI, Microsoft Semantic Kernel, custom enterprise implementations, and simple Python-based agents without requiring framework replacement or extensive refactoring.',
  
  '7-NVDA-Platform-Integration/10-AIQ': 'NVIDIA Agent Intelligence Toolkit represents a flexible lightweight framework for connecting enterprise agents to data sources and tools across heterogeneous agent framework ecosystems. The toolkit provides unified integration layer enabling agents built with diverse frameworks to access common capabilities, share tools and workflows, and operate cohesively within complex multi-agent systems.',
  
  '7-NVDA-Platform-Integration/11-Mastering-LLMs': 'Large language model inference optimization encompasses comprehensive strategies and techniques for reducing computational costs, memory consumption, and latency when deploying transformer-based models in production environments. The optimization framework spans multiple complementary approaches operating across different system layers from low-level memory management through algorithmic efficiency to high-level serving orchestration.',
  
  '7-NVDA-Platform-Integration/12-Deploy-Inference-Workloads': 'NVIDIA NIM inference workload deployment represents comprehensive framework for serving production-scale generative AI models through managed inference infrastructure. The deployment architecture provides complete setup and configuration specifications including container image selection, dataset connectivity, network configuration, and resource allocation required for serving trained models in real-time or batch prediction scenarios.',
  
  '7-NVDA-Platform-Integration/13-Nemotron-advanced-agents': 'NVIDIA Llama Nemotron API represents programmatic interface for accessing family of reasoning-optimized large language models designed for advanced AI agent development and complex cognitive tasks. The API exposes models built on Meta\'s Llama framework enhanced through post-training techniques including distillation and reinforcement learning, delivering capabilities excelling at scientific analysis, advanced mathematics, coding, and instruction following.',
  
  '7-NVDA-Platform-Integration/14-AI-Agent-blueprint': 'AI-Q NVIDIA Blueprint represents open-source reference architecture for building artificial general agents connecting to enterprise data sources, reasoning across multimodal information, and delivering comprehensive accurate answers at scale. The architecture integrates three foundational building blocks enabling robust scalable reliable agent development across domains and industries.',
  
  '7-NVDA-Platform-Integration/15-Improve-AI-code-gen': 'AI code generation with NeMo Agent Toolkit represents application of agentic architectures to automated software development tasks leveraging test-time computation scaling and reasoning model integration. Test-driven development integration creates feedback loops where agents generate code, execute tests against generated implementations, analyze failures through reasoning models, and iteratively refine solutions until tests pass.',
  
  '7-NVDA-Platform-Integration/16-Nemo-Scalable-Ai': 'NVIDIA NeMo Framework represents scalable cloud-native generative AI development platform built for researchers and practitioners working across large language models, multimodal models, automatic speech recognition, text-to-speech synthesis, and computer vision domains. The framework enables efficient creation, customization, and deployment of generative AI models through leveraging existing code repositories and pre-trained model checkpoints.',

  // Section 8: Run, Monitor, Maintain
  '8-run-monitor-maintain/1-AI-Agent-Eval': 'AI agent evaluation represents systematic assessment processes for understanding performance characteristics, decision-making quality, and interaction effectiveness of autonomous AI systems executing tasks and serving users. The evaluation scope extends beyond traditional text generation quality metrics including coherence, relevance, and faithfulness used for standard large language model benchmarks.',
  
  '8-run-monitor-maintain/2-Log-Trace-Monitor': 'LLM logging and tracing represents observability framework for tracking, correlating, and analyzing API calls within generative AI applications where single user requests trigger multiple backend operations. Trace correlation mechanisms associate related API calls through common identifiers enabling reconstruction of complete request flows from disparate operations.',
  
  '8-run-monitor-maintain/3-Time-Weighted-Retriever': 'Time-weighted retrieval represents enhancement to semantic similarity search incorporating temporal recency factors into document ranking algorithms. Temporal weighting mechanisms modify similarity scores through decay functions diminishing relevance of documents based on time since last access.',
  
  '8-run-monitor-maintain/4-Troubleshooting': 'TensorRT-LLM debugging and troubleshooting represents comprehensive methodology for identifying, diagnosing, and resolving issues arising during compilation, model building, execution, and deployment of optimized large language model inference engines. Debugging capabilities enable developers to inspect intermediate computation states, validate tensor shapes and values throughout execution pipelines, and verify correctness of optimizations applied during engine compilation.',
  
  '8-run-monitor-maintain/5-Langchain-Tracing': 'LangChain represents open-source framework providing pre-built agent architectures and model integrations enabling rapid development of autonomous applications powered by large language models. The framework abstracts complexities of model provider APIs, tool integration, and agent orchestration through standardized interfaces and reusable components.',
  
  '8-run-monitor-maintain/6-LangChain-structured-outputs': 'LangChain represents open-source framework providing pre-built agent architectures and model integrations enabling rapid development of autonomous applications powered by large language models. Model integration layer provides unified interface across diverse LLM providers including commercial APIs and open-source models.',
  
  '8-run-monitor-maintain/7-Smith-Langchain-eval': 'LangChain represents open-source framework providing pre-built agent architectures and model integrations enabling rapid development of autonomous applications powered by large language models. The framework abstracts complexities of model provider APIs, tool integration, and agent orchestration through standardized interfaces and reusable components.',
  
  '8-run-monitor-maintain/8-Monitoring-ML-production': 'Machine learning model monitoring represents systematic tracking and analysis of deployed model behavior ensuring continued performance alignment with expectations despite dynamic real-world conditions. Monitoring proves essential as models experience degradation or failures in production environments where data distributions shift, system conditions vary, and operational contexts evolve beyond training scenario assumptions.',

  // Section 9: Safety and Ethics
  '9-Safety-Ethics/1-Building-Safer-Apps-Templates': 'LangChain Templates and NeMo Guardrails integration represents architectural pattern combining reusable application templates with programmable safety controls enabling rapid development of trustworthy LLM-powered applications. Templates provide pre-configured agent and chain implementations accelerating development through proven architectures, while guardrails layer ensures responses remain accurate, secure, and contextually appropriate through runtime validation and content moderation.',
  
  '9-Safety-Ethics/2-AI-ML-Software-Med-Device': 'AI/ML technologies in medical devices represent machine-based systems making predictions, recommendations, or decisions influencing healthcare delivery through automated analysis of clinical data. Medical device applications span diagnostic systems analyzing imaging data to identify pathologies, monitoring devices estimating clinical event probabilities from physiological signals, and therapeutic systems adapting interventions based on patient responses.',
  
  '9-Safety-Ethics/3-Proposed-Regulation-Harmonized': 'The EU AI Act represents harmonized regulatory framework establishing rules for artificial intelligence system development, deployment, and usage across European Union member states. Risk-based regulatory approach tailors requirements proportionally to potential harms where AI systems creating unacceptable risks face prohibition, high-risk systems require compliance with mandatory requirements and conformity assessment, and low-risk systems face minimal transparency obligations.',
  
  '9-Safety-Ethics/4-Ethically-Alinged-Design': 'AI ethics standards framework represents systematic approach to embedding ethical considerations throughout artificial intelligence system lifecycles addressing risks and responsibilities emerging from autonomous intelligent systems, machine learning, and robotics deployment. Ethical considerations encompass multiple dimensions including human dignity preservation, rights protection, societal benefit maximization, and harm prevention.',
  
  '9-Safety-Ethics/5-Securing-Gen-AI-Deployments': 'NIM and NeMo Guardrails integration represents architectural pattern combining high-performance AI model inference microservices with programmable safety controls enabling secure enterprise deployment of generative AI applications. NIM microservices architecture delivers optimized inference through prebuilt containers encapsulating models, inference engines, and standard APIs.',
  
  '9-Safety-Ethics/6-Metrics-Agentic-Ai': 'Agentic tool use evaluation represents systematic assessment methodologies for measuring autonomous agent performance across multiple dimensions including goal achievement, tool selection accuracy, conversation management, and topic adherence. Multi-dimensional assessment recognizes agent quality emerges from multiple factors beyond simple accuracy metrics.',
  
  '9-Safety-Ethics/7-AI-Regulatory': 'AI for regulatory compliance represents application of machine learning, natural language processing, predictive analytics, and robotic process automation to automate, enhance, and optimize organizational adherence to laws, regulations, and industry standards. Compliance automation encompasses document analysis extracting requirements from regulatory text, transaction monitoring detecting violations in real-time, reporting generation assembling required disclosures, and change management tracking regulatory updates.',
  
  '9-Safety-Ethics/8-Responsible-Ai': 'Responsible AI represents comprehensive framework for ensuring artificial intelligence systems operate ethically, securely, and transparently throughout their lifecycle. The framework addresses critical considerations including fairness, accountability, transparency, and safety that prove essential for building trust and ensuring AI systems deliver value without causing harm.',

  // Section 10: Human-AI Interaction
  '10-Human-AI-Interaction/1-Data-Flywheel': 'AI data flywheel represents a self-improving continuous loop where data collected from system interactions feeds model refinement, generating progressively better outcomes and higher-quality training data. The flywheel mechanism creates a virtuous cycle where improved models generate better predictions, producing more valuable feedback data that enables further improvements.',
  
  '10-Human-AI-Interaction/2-AI-Agents-HITl': 'Traditional customer care implementations have long utilized chatbots, functioning similarly to how Interactive Voice Response systems have been used, but with text-based interactions instead of voice. However, LangChain\'s latest work demonstrates how human oversight can be effectively integrated as checkpoints before tasks are executed, addressing concerns about autonomous agentic applications.',
  
  '10-Human-AI-Interaction/3-HITL-Holistic': 'Human-in-the-Loop AI addresses the need for human oversight and accountability. Unlike fully autonomous AI systems that operate without human intervention, Human-in-the-Loop AI involves humans at critical stages—from data annotation to continuous feedback and decision-making—ensuring that AI outputs align with human values.',
  
  '10-Human-AI-Interaction/4-HITL-One-Reach': 'Agentic AI is transforming industries, delivering measurable improvements in efficiency, insight, and scalability. Human-in-the-Loop in Agentic AI systems is about humans and agents working hand in hand, where people stay involved at key moments such as checking outputs, fixing errors, and guiding learning, rather than letting AI agents run completely on their own.',
  
  '10-Human-AI-Interaction/5-Aporia-Ai-Guardrails': 'Artificial Intelligence has made tremendous strides in recent years, transforming industries and making our lives easier. AI guardrails are policies and frameworks designed to ensure that Large Language Models operate within ethical, legal, and technical boundaries, preventing AI from causing harm, making biased decisions, or being misused.',
  
  '10-Human-AI-Interaction/6-CoT-prompting': 'While working with a large language model like ChatGPT or Gemini AI, we often run into situations where the model gives a wrong answer. Chain of Thought prompting enables LLM models to perform complex reasoning tasks by forcing the model to break them down into step-by-step logical sequences.',

  // Review Files - Comprehensive Category Reviews
  '1-agent-architecture-design/review': 'This comprehensive review explores the fundamental principles and architectural patterns for designing effective AI agent systems, covering everything from basic agent structures to complex multi-agent orchestration. The review examines key concepts including agent autonomy, reasoning mechanisms, memory management, knowledge integration, and scalability considerations that form the foundation of modern agentic AI systems.',
  
  '2-agent-development/review': 'This comprehensive review covers the essential development practices and tools for building production-ready AI agents, from optimization techniques and framework integration to prompt engineering and multimodal capabilities. The review examines critical development patterns including fault handling, retry strategies, circuit breakers, and evaluation methodologies that ensure robust and reliable agent implementations.',
  
  '3-evaluation-tuning/review': 'This comprehensive review explores systematic approaches to evaluating, monitoring, and improving AI agent performance throughout their lifecycle, from initial development through production deployment. The review examines evaluation frameworks, observability tools, tuning techniques, and continuous improvement strategies that enable organizations to build trustworthy and high-performing agentic systems.',
  
  '4-deployment-scaling/review': 'This comprehensive review covers enterprise-scale deployment strategies for AI agents, focusing on Kubernetes orchestration, performance optimization, monitoring infrastructure, and scaling techniques. The review examines critical infrastructure components including storage systems, inference servers, performance analysis tools, and monitoring stacks that enable reliable and scalable AI agent deployments.',
  
  '5-cognition-planning-memory/review': 'This comprehensive review explores the cognitive capabilities that enable AI agents to reason, plan, and remember, covering advanced techniques for context management, reinforcement learning, and memory architectures. The review examines how agents process information, make decisions, maintain state across interactions, and adapt their behavior based on experience and feedback.',
  
  '6-knowledge-integration-data-handling/review': 'This comprehensive review covers techniques for integrating external knowledge into AI agents, focusing on retrieval-augmented generation, fine-tuning strategies, and data handling approaches. The review examines how agents access, process, and utilize information from various sources to enhance their capabilities and provide accurate, grounded responses.',
  
  '7-NVDA-Platform-Integration/review': 'This comprehensive review explores NVIDIA platform tools and frameworks for building, optimizing, and deploying AI agents, covering TensorRT optimization, Triton Inference Server, NeMo Framework, and NeMo Guardrails. The review examines how these platform components work together to provide a complete ecosystem for developing high-performance, production-ready agentic AI systems.',
  
  '8-run-monitor-maintain/review': 'This comprehensive review covers operational practices for running, monitoring, and maintaining AI agents in production environments, focusing on evaluation methodologies, logging and tracing, troubleshooting techniques, and continuous monitoring strategies. The review examines how organizations ensure reliability, performance, and compliance throughout the agent lifecycle.',
  
  '9-Safety-Ethics/review': 'This comprehensive review explores critical considerations for building safe, ethical, and compliant AI agents, covering regulatory frameworks, safety guardrails, ethical design principles, and responsible AI practices. The review examines how organizations balance innovation with safety, ensuring that agentic systems operate within ethical boundaries and regulatory requirements.',
  
  '10-Human-AI-Interaction/review': 'This comprehensive review covers the design principles and interaction patterns that enable effective collaboration between humans and AI agents, focusing on human-in-the-loop systems, data flywheels, guardrails, and prompting techniques. The review examines how to create intuitive, trustworthy, and productive human-agent interfaces that enhance rather than replace human capabilities.',
};

// Helper function to normalize article path for lookup
export function normalizeArticlePathForSummary(articlePath: string): string {
  let normalized = articlePath;
  // Remove .txt extension
  normalized = normalized.replace(/\.txt$/, '');
  // Remove common path prefixes
  normalized = normalized.replace(/^summaries\//, '');
  normalized = normalized.replace(/^app\/data\/content\/summaries\//, '');
  return normalized;
}

// Helper function to get article summary
export function getArticleSummary(articlePath: string): string | null {
  const normalizedPath = normalizeArticlePathForSummary(articlePath);
  return ARTICLE_SUMMARIES[normalizedPath] || null;
}
