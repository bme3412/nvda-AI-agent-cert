Overview

NeMo RL is an open-source post-training library within the NeMo Framework, designed to streamline and scale reinforcement learning methods for multimodal models, including LLMs and VLMs. Designed for flexibility, reproducibility, and scale, NeMo RL enables both small-scale experiments and massive multi-GPU, multi-node deployments for fast experimentation in research and production environments.

What You Can Expect

NeMo RL provides several key capabilities that make it suitable for a wide range of reinforcement learning applications. The library offers flexibility with a modular design that allows easy integration and customization, enabling researchers and developers to adapt the framework to their specific needs. Efficient resource management is achieved using Ray, enabling scalable and flexible deployment across different hardware configurations. The framework is hackable with native PyTorch-only paths for quick research prototypes, allowing rapid iteration and experimentation. High performance is delivered through Megatron Core, supporting various parallelism techniques for large models and large context lengths. Seamless integration with Hugging Face provides ease of use, allowing users to leverage a wide range of pre-trained models and tools. The library includes comprehensive documentation that is both detailed and user-friendly, with practical examples to guide implementation.

Quick Start

The quick start guide enables users to get started with either the native PyTorch DTensor or Megatron Core training backends. Both training backends are independent, meaning users can install and use either one on its own. To begin, users should clone the repository and create the environment by running git clone, navigating to the directory, updating submodules, and creating a virtual environment. If users previously ran without checking out the submodules, they may need to rebuild virtual environments by setting NRL_FORCE_REBUILD_VENVS=true.

For the native PyTorch DTensor backend, users can run GRPO by executing the command uv run python examples/run_grpo_math.py. For the Megatron Core backend, users can run GRPO using the command uv run examples/run_grpo_math.py with the configuration file examples/configs/grpo_math_1B_megatron.yaml. For more examples and setup details, users should continue to the Prerequisites section.

Features and Roadmap

NeMo RL provides a comprehensive set of features currently available, with additional capabilities planned for future releases. Available now, the library includes distributed training with Ray-based infrastructure, enabling scalable deployment across multiple nodes and GPUs. Environment support and isolation provides support for multi-environment training and dependency isolation between components. Worker isolation ensures process isolation between RL Actors, eliminating concerns about global state. The library supports multiple learning algorithms including GRPO, GSPO, SFT, and DPO. Multi-turn RL enables multi-turn generation and training for RL with tool use, games, and other complex scenarios. Advanced parallelism with DTensor supports PyTorch FSDP2, TP, CP, and SP for efficient training. Larger model support with longer sequences is achieved through performant parallelisms with Megatron Core, including TP, PP, CP, SP, EP, and FSDP. MoE models are supported, including DeepSeekV3 and Qwen-3 MoE models through the Megatron backend. Sequence packing is available in both DTensor and Megatron Core for significant training performance gains. Fast generation is provided through the vLLM backend for optimized inference. Hugging Face integration works with models ranging from 1B to 70B parameters, including Qwen and Llama models.

Coming in version 0.4, the library will include Megatron Inference for fast Day-0 support for new Megatron models, avoiding weight conversion. Async RL will provide support for asynchronous rollouts and replay buffers for off-policy training, enabling a fully asynchronous GRPO. Vision Language Models (VLM) support will enable SFT and GRPO on VLMs through the DTensor path. Improved native performance will enhance training time for native PyTorch models. Improved large MoE performance will enhance Megatron Core training performance and generation performance. End-to-end FP8 low-precision training will provide support for Megatron Core FP8 training and FP8 vLLM generation. Megatron Bridge integration will integrate Megatron Bridge to enable training features from Megatron Core. NeMo Automodel integration will integrate NeMo Automodel to power the DTensor path. New models will include gpt-oss. Algorithm expansion will add DAPO, GSPO, and On-policy Distillation. GB200 container support will be added for GB200 hardware.

Training and Generation Backends

NeMo RL supports multiple training backends to accommodate different model sizes and hardware configurations. The PyTorch backend leverages NeMo AutoModel to provide accelerated PyTorch training with improved memory efficiency, supporting PyTorch-native TP, SP, PP, CP, and FSDP2. The Megatron backend provides NVIDIA's high-performance training framework for scaling to large models with 6D parallelisms. The training backend is automatically determined based on YAML configuration settings. For detailed information on backend selection, configuration, and examples, users should refer to the Training Backends documentation.

NeMo RL also supports multiple generation and rollout backends to accommodate different model sizes and hardware configurations. The vLLM backend provides a high-throughput and memory-efficient popular inference and serving engine. The Megatron backend provides a high-performance Megatron-native inference backend which eliminates weight conversion between training and inference. For detailed information on backend selection, configuration, and examples, users should refer to the Generation Backends documentation.

Algorithms

NeMo RL supports multiple training algorithms for post-training large language models. The library provides comprehensive support for various algorithms across both single-node and multi-node configurations. GRPO is supported on both single-node and multi-node setups, with examples including GRPO Qwen2.5-32B and GRPO Multi-Turn. DAPO is supported with similar examples to GRPO for both single-node and multi-node configurations. On-policy distillation is supported on both single-node and multi-node setups, and is also supported in the PyTorch DTensor path. Supervised Fine-Tuning (SFT) is available for both single-node and multi-node deployments. DPO is supported across both single-node and multi-node configurations. Reward Model (RM) training is available for both single-node and multi-node setups. This comprehensive algorithm support matrix enables users to select the appropriate algorithm and deployment configuration based on their specific requirements and infrastructure.

Evaluation

NeMo RL provides evaluation tools to assess model capabilities. If users have trained a model and saved the checkpoint in the PyTorch DCP format, they first need to convert it to the Hugging Face format before running evaluation. For example, for a GRPO checkpoint at step 170, users can run the conversion script with the configuration file, DCP checkpoint path, and target Hugging Face checkpoint path. If users have a model saved in Megatron format, they can use a conversion command to convert it to Hugging Face format prior to running evaluation. This script requires Megatron Core, so users must make sure they launch with the mcore extra. Users should adjust the paths according to their training output directory structure. For an in-depth explanation of checkpointing, users should refer to the Checkpointing documentation.

To run evaluation, users can execute the evaluation script with the converted model, specifying the model name path. Users can also run the evaluation script with custom settings, such as evaluating a specific model on a particular dataset with custom generation parameters. For example, users can evaluate DeepScaleR-1.5B-Preview on MATH-500 using 8 GPUs, with Pass@1 accuracy averaged over 16 samples for each problem, by specifying the configuration file, model name, temperature, top_p, maximum model length, dataset name, number of tests per prompt, and GPUs per node. Evaluation results may vary slightly due to various factors, such as sampling parameters, random seed, inference engine version, and inference engine settings. Users should refer to the evaluation configuration file for a full list of parameters that can be overridden. For an in-depth explanation of evaluation, users should refer to the Evaluation documentation.
