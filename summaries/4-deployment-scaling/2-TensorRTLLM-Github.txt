TensorRT LLM
TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.
Documentation python python cuda torch version license

Architecture   |   Performance   |   Examples   |   Documentation   |   Roadmap

Tech Blogs
[10/13] Scaling Expert Parallelism in TensorRT LLM (Part 3: Pushing the Performance Boundary) ‚ú® ‚û°Ô∏è link

[09/26] Inference Time Compute Implementation in TensorRT LLM ‚ú® ‚û°Ô∏è link

[09/19] Combining Guided Decoding and Speculative Decoding: Making CPU and GPU Cooperate Seamlessly ‚ú® ‚û°Ô∏è link

[08/29] ADP Balance Strategy ‚ú® ‚û°Ô∏è link

[08/05] Running a High-Performance GPT-OSS-120B Inference Server with TensorRT LLM ‚ú® ‚û°Ô∏è link

[08/01] Scaling Expert Parallelism in TensorRT LLM (Part 2: Performance Status and Optimization) ‚ú® ‚û°Ô∏è link

[07/26] N-Gram‚ÄØSpeculative‚ÄØDecoding‚ÄØin TensorRT LLM ‚ú® ‚û°Ô∏è link

[06/19] Disaggregated Serving in TensorRT LLM ‚ú® ‚û°Ô∏è link

[06/05] Scaling Expert Parallelism in TensorRT LLM (Part 1: Design and Implementation of Large-scale EP) ‚ú® ‚û°Ô∏è link

[05/30] Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers ‚ú® ‚û°Ô∏è link

[05/23] DeepSeek R1 MTP Implementation and Optimization ‚ú® ‚û°Ô∏è link

[05/16] Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs ‚ú® ‚û°Ô∏è link

Latest News
[08/05] üåü TensorRT LLM delivers Day-0 support for OpenAI's latest open-weights models: GPT-OSS-120B ‚û°Ô∏è link and GPT-OSS-20B ‚û°Ô∏è link

[07/15] üåü TensorRT LLM delivers Day-0 support for LG AI Research's latest model, EXAONE 4.0 ‚û°Ô∏è link

[06/17] Join NVIDIA and DeepInfra for a developer meetup on June 26 ‚ú® ‚û°Ô∏è link

[05/22] Blackwell Breaks the 1,000 TPS/User Barrier With Meta‚Äôs Llama 4 Maverick ‚ú® ‚û°Ô∏è link

[04/10] TensorRT LLM DeepSeek R1 performance benchmarking best practices now published. ‚ú® ‚û°Ô∏è link

[04/05] TensorRT LLM can run Llama 4 at over 40,000 tokens per second on B200 GPUs!

L4_perf

[03/22] TensorRT LLM is now fully open-source, with developments moved to GitHub!

[03/18] üöÄüöÄ NVIDIA Blackwell Delivers World-Record DeepSeek-R1 Inference Performance with TensorRT LLM ‚û°Ô∏è Link

[02/28] üåü NAVER Place Optimizes SLM-Based Vertical Services with TensorRT LLM ‚û°Ô∏è Link

[02/25] üåü DeepSeek-R1 performance now optimized for Blackwell ‚û°Ô∏è Link

[02/20] Explore the complete guide to achieve great accuracy, high throughput, and low latency at the lowest cost for your business here.

[02/18] Unlock #LLM inference with auto-scaling on @AWS EKS ‚ú® ‚û°Ô∏è link

[02/12] ü¶∏‚ö° Automating GPU Kernel Generation with DeepSeek-R1 and Inference Time Scaling ‚û°Ô∏è link

[02/12] üåü How Scaling Laws Drive Smarter, More Powerful AI ‚û°Ô∏è link

Previous News
TensorRT LLM Overview
TensorRT LLM is an open-sourced library for optimizing Large Language Model (LLM) inference. It provides state-of-the-art optimizations, including custom attention kernels, inflight batching, paged KV caching, quantization (FP8, FP4, INT4 AWQ, INT8 SmoothQuant, ...), speculative decoding, and much more, to perform inference efficiently on NVIDIA GPUs.

Architected on PyTorch, TensorRT LLM provides a high-level Python LLM API that supports a wide range of inference setups - from single-GPU to multi-GPU or multi-node deployments. It includes built-in support for various parallelism strategies and advanced features. The LLM API integrates seamlessly with the broader inference ecosystem, including NVIDIA Dynamo and the Triton Inference Server.

TensorRT LLM is designed to be modular and easy to modify. Its PyTorch-native architecture allows developers to experiment with the runtime or extend functionality. Several popular models are also pre-defined and can be customized using native PyTorch code, making it easy to adapt the system to specific needs.

Getting Started
To get started with TensorRT-LLM, visit our documentation:

Quick Start Guide
Running DeepSeek
Installation Guide for Linux
Installation Guide for Grace Hopper
Supported Hardware, Models, and other Software
Benchmarking Performance
Release Notes
Deprecation Policy
Deprecation is used to inform developers that some APIs and tools are no longer recommended for use. Beginning with version 1.0, TensorRT LLM has the following deprecation policy:

Communication of Deprecation
Deprecation notices are documented in the Release Notes.
Deprecated APIs, methods, classes, or parameters include a statement in the source code indicating when they were deprecated.
If used, deprecated methods, classes, or parameters issue runtime deprecation warnings.
Migration Period
TensorRT LLM provides a 3-month migration period after deprecation.
During this period, deprecated APIs, tools, or parameters continue to work but trigger warnings.
Scope of Deprecation
Full API/Method/Class Deprecation: The entire API/method/class is marked for removal.
Partial Deprecation: If only specific parameters of an API/method are deprecated (e.g., param1 in LLM.generate(param1, param2)), the method itself remains functional, but the deprecated parameters will be removed in a future release.
Removal After Migration Period
After the 3-month migration period ends, deprecated APIs, tools, or parameters are removed in a manner consistent with semantic versioning (major version changes may include breaking removals).
