Overview

NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails, or rails for short, provide a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. While there are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training through model alignment, NeMo Guardrails takes a different approach. Using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications that are user-defined, independent of the underlying LLM, and interpretable. Initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.

Introduction

Steerability and trustworthiness are key factors for deploying Large Language Models (LLMs) in production. Enabling these models to stay on track for multiple turns of a conversation is essential for developing task-oriented dialogue systems. This presents a serious challenge as LLMs can be easily led into veering off-topic. At the same time, LLMs also tend to generate responses that are factually incorrect or completely fabricated, known as hallucinations. In addition, they are vulnerable to prompt injection or jailbreak attacks, where malicious actors manipulate inputs to trick the model into producing harmful outputs.

Building trustworthy and controllable conversational systems is of vital importance for deploying LLMs in customer-facing situations. NeMo Guardrails is an open-source toolkit for easily adding programmable rails to LLM-based applications. Guardrails provide a mechanism for controlling the output of an LLM to respect some human-imposed constraints, such as not engaging in harmful topics, following a predefined dialogue path, adding specific responses to some user requests, using a particular language style, or extracting structured data. To implement the various types of rails, several techniques can be used, including model alignment at training, prompt engineering and chain-of-thought (CoT), and adding a dialogue manager. While model alignment provides general rails embedded in the LLM at training and prompt tuning can offer user-specific rails embedded in a customized model, NeMo Guardrails allows users to define custom programmable rails at runtime. This mechanism is independent of alignment strategies and supplements embedded rails, works with different LLMs, and provides interpretable rails defined using a custom modeling language called Colang.

To implement user-defined programmable rails for LLMs, the toolkit uses a programmable runtime engine that acts like a proxy between the user and the LLM. This approach is complementary to model alignment and defines the rules the LLM should follow in the interaction with users. The Guardrails runtime has the role of a dialogue manager, being able to interpret and impose the rules defining the programmable rails. These rules are expressed using a modeling language called Colang. More specifically, Colang is used to define rules as dialogue flows that the LLM should always follow. Using a prompting technique with in-context learning and a specific form of CoT, the system enables the LLM to generate the next steps that guide the conversation. Colang is then interpreted by the dialogue manager to apply the guardrails rules predefined by users or automatically generated by the LLM to guide the behavior of the LLM.

While NeMo Guardrails can be used to add safety and steerability to any LLM-based application, dialogue systems powered by an LLM benefit the most from using Colang and the Guardrails runtime. The toolkit is licensed as Apache 2.0, and provides initial support for several LLM providers, together with starter example applications and evaluation tools.

Related Work

Existing solutions for adding rails to LLMs rely heavily on model alignment techniques such as instruction-tuning or reinforcement learning. The alignment of LLMs works on several dimensions, mainly to improve helpfulness and to reduce harmfulness. Alignment in general, including red-teaming, requires a large collection of input prompts and responses that are manually labeled according to specific criteria such as harmlessness. Model alignment provides rails embedded at training in the LLM that cannot easily be changed at runtime by users. Moreover, it also requires a large set of human-annotated response ratings for each rail to be incorporated by the LLM. While Reinforcement Learning from Human Feedback (RLHF) is the most popular method for model alignment, alternatives such as RL from AI Feedback do not require a human labeled dataset and use the actual LLM to provide feedback for each response. While most alignment methods provide general embedded rails, developers can add app-specific embedded rails to an LLM via prompt tuning.

The most common approach to add user-defined programmable rails to an LLM is to use prompting, including prompt engineering and in-context learning, by prepending or appending a specific text to the user input. This text specifies the behavior that the LLM should adhere to. The other approach to provide LLMs with user-defined runtime rails is to use chain-of-thought (CoT). In its simplest form, CoT appends to the user instruction one or several similar examples of input and output pairs for the task at hand. Each of these examples contains a more detailed explanation in the output, useful for determining the final answer. Other more complex approaches involve several steps of prompting the LLM in a generic to specific way or even with entire dialogues with different roles similar to an inner monologue.

Building task-oriented dialogue agents generally requires two components: a Natural Language Understanding (NLU) and a Dialogue Management (DM) engine. There exist a wide range of tools and solutions for both NLU and DM, ranging from open-source solutions like Rasa to proprietary platforms such as Microsoft LUIS or Google DialogFlow. Their functionality mostly follows these two steps: first the NLU extracts the intent and slots from the user message, then the DM predicts the next dialogue state given the current dialogue context. The set of intents and dialogue states are finite and pre-defined by a conversation designer. The bot responses are also chosen from a closed set depending on the dialogue state. This approach allows defining specific dialogue flows that tightly control any dialogue agent. Conversely, these agents are rigid and require a high amount of human effort to design and update the NLU and dialogue flows. At the other end of the spectrum are recent end-to-end (E2E) generative approaches that use LLMs for dialogue tracking and bot message generation. NeMo Guardrails also uses an E2E approach to build LLM-powered dialogue agents, but it combines a DM-like runtime able to interpret and maintain the state of dialogue flows written in Colang with a CoT-based approach to generate bot messages and even new dialogue flows using an LLM.

NeMo Guardrails Architecture

NeMo Guardrails acts like a proxy between the user and the LLM. It allows developers to define programmatic rails that the LLM should follow in the interaction with users using Colang, a formal modeling language designed to specify flows of events, including conversations. Colang is interpreted by the Guardrails runtime which applies the user-defined rules or automatically generated rules by the LLM. These rules implement the guardrails and guide the behavior of the LLM.

Colang scripts are at the core of a Guardrails app configuration. The main elements of a Colang script are user canonical forms, dialogue flows, and bot canonical forms. All these three types of definitions are also indexed in a vector database, such as Annoy or FAISS, to allow for efficient nearest-neighbors lookup when selecting the few-shot examples for the prompt. The interaction between the LLM and the Guardrails runtime is defined using Colang rules. When prompted accordingly, the LLM is able to generate Colang-style code using few-shot in-prompt learning. Otherwise, the LLM works in normal mode and generates natural language.

Canonical forms are a key mechanism used by Colang and the runtime engine. They are expressed in natural language, such as English, and encode the meaning of a message in a conversation, similar to an intent. The main difference between intents and canonical forms is that the former are designed as a closed set for a text classification task, while the latter are generated by an LLM and thus are not bound in any way, but are guided by the canonical forms defined by the Guardrails app. The set of canonical forms used to define the rails that guide the interaction is specified by the developer; these are used to select few-shot examples when generating the canonical form for a new user message.

Using these key concepts, developers can implement a variety of programmable rails. There are two main categories: topical rails and execution rails. Topical rails are intended for controlling the dialogue, such as to guide the response for specific topics or to implement complex dialogue policies. Execution rails call custom actions defined by the app developer; the toolkit focuses on a set of safety rails available to all Guardrails apps.

Topical Rails

Topical rails employ the key mechanism used by NeMo Guardrails: Colang for describing programmable rails as dialogue flows, together with the Colang interpreter in the runtime for dialogue management. Flows are specified by the developer to determine how the user conversation should proceed. The dialogue manager in the Guardrails runtime uses an event-driven design, an event loop that processes events and generates back other events, to ensure which flows are active in the current dialogue context.

The runtime has three main stages for guiding the conversation with dialogue flows and thus ensuring the topical rails. First, using similarity-based few-shot prompting, the system generates the canonical form for each user input, allowing the guardrails system to trigger any user-defined flows. Second, once the user canonical form is identified, there are two potential paths. If the canonical form matches any of the developer-specified flows, the next step is extracted from that particular flow by the dialogue manager. For user canonical forms that are not defined in the current dialogue context, the system uses the generalization capability of the LLM to decide the appropriate next steps. For example, for a travel reservation system, if a flow is defined for booking bus tickets, the LLM should generate a similar flow if the user wants to book a flight. Third, conditioned by the next step, the LLM is prompted to generate a response. Thus, if the system does not want the bot to respond to political questions, and the next step for such a question is bot inform cannot answer, the bot would deflect from responding, respecting the rail.

Execution Rails

The toolkit also makes it easy to add execution rails. These are custom actions, defined in Python, monitoring both the input and output of the LLM, and can be executed by the Guardrails runtime when encountered in a flow. While execution rails can be used for a wide range of tasks, the toolkit provides several rails for LLM safety covering fact-checking, hallucination, and moderation.

The fact-checking rail operates under the assumption of retrieval augmented generation, formulating the task as an entailment problem. Specifically, given an evidence text and a generated bot response, the system asks the LLM to predict whether the response is grounded in and entailed by the evidence. For each evidence-hypothesis pair, the model must respond with a binary entailment prediction. If the model predicts that the hypothesis is not entailed by the evidence, this suggests the generated response may be incorrect. Different approaches can be used to handle such situations, such as abstaining from providing an answer.

For general-purpose questions that do not involve a retrieval component, the toolkit defines a hallucination rail to help prevent the bot from making up facts. The rail uses self-consistency checking similar to SelfCheckGPT: given a query, the system first samples several answers from the LLM and then checks if these different answers are in agreement. For hallucinated statements, repeated sampling is likely to produce responses that are not in agreement. After obtaining n samples from the LLM for the same prompt, the system concatenates n minus 1 responses to form the context and uses the nth response as the hypothesis. Then the system uses the LLM to detect if the sampled responses are consistent using a specific prompt template.

The moderation process in NeMo Guardrails contains two key components. Input moderation, also referred to as jailbreak rail, aims to detect potentially malicious user messages before reaching the dialogue system. Output moderation aims to detect whether the LLM responses are legal, ethical, and not harmful prior to being returned to the user. The moderation system functions as a pipeline, with the user message first passing through input moderation before reaching the dialogue system. After the dialogue system generates a response powered by an LLM, the output moderation rail is triggered. Only after passing both moderation rails, the response is returned to the user. Both the input and output moderation rails are framed as another task to a powerful, well-aligned LLM that vets the input or response.

Sample Guardrails Applications

Adding rails to conversation applications is simple and straightforward using Colang scripts. Topical rails can be used in combination with execution rails to decide when a specific action should be called or to define complex dialogue flows for building task-oriented agents. For example, two topical rails can be implemented that allow the Guardrails app to use the WolframAlpha engine to respond to math and distance queries. To achieve this, the wolfram alpha request custom action, implemented in Python and available on Github, uses the WolframAlpha API to get a response to the user query. This response is then used by the LLM to generate an answer in the context of the current conversation.

The steps involved in adding execution rails are as follows. First, defining a rail requires the developer to define an action that specifies the logic for the rail in Python. Second, once the action has been defined, it can be called from Colang using the execute keyword. Third, the developer can specify how the application should react to the output from the action. For example, a sample flow in Colang can invoke the check_jailbreak action. If the jailbreak rail flags a user message, the developer can decide not to show the generated response and to output a default text instead.

Evaluation

The evaluation of topical rails focuses on the core mechanism used by the toolkit to guide conversations using canonical forms and dialogue flows. The current evaluation experiments employ datasets used for conversational NLU. Starting from an NLU dataset, a Colang application can be created by mapping intents to canonical forms and defining simple dialogue flows for them. The evaluation dataset used in experiments is balanced, containing at most 3 samples per intent sampled randomly from the original datasets. The test dataset has 231 samples spanning over 77 different intents.

Results show that topical rails can be successfully used to guide conversations even with smaller open source models such as falcon-7b-instruct or llama2-13b-chat. As the performance of an LLM is heavily dependent on the prompt, all results might be improved with better prompting. The topical rails evaluation highlights several important aspects. First, each step in the three-step approach, user canonical form, next step, and bot message, used by Guardrails offers an improvement in performance. Second, it is important to have at least k equals 3 samples in the vector database for each user canonical form for achieving good performance. Third, some models, such as gpt-3.5-turbo, produce a wider variety of canonical forms, even with few-shot prompting. In these cases, it is useful to add a similarity match instead of exact match for generating canonical forms.

To evaluate the moderation rails, the system uses the Anthropic Red-Teaming and Helpful datasets. A balanced harmful-helpful evaluation set is created by sampling prompts with the highest harmful score from the Red-Teaming dataset, while selecting an equal number of prompts from the Helpful dataset. The performance of the rails is quantified based on the proportion of harmful prompts that are blocked and the proportion of helpful ones that are allowed. Analysis of the results shows that using both the input and output moderation rails is much more robust than using either one of the rails individually. Using both rails, gpt-3.5-turbo has great performance, blocking close to 99 percent of harmful prompts, compared to 93 percent without the rails, and just 2 percent of helpful requests.

For the fact-checking rail evaluation, the system considers the MSMARCO dataset, which consists of context, question, and answer triples. In order to mine negatives, answers that are not grounded in the context, the system uses OpenAI text-davinci-003 to rewrite the positive answer to a hard negative that looks similar to it, but is not grounded in the evidence. A combined dataset is constructed by equally sampling both positive and negative triples. Both text-davinci-003 and gpt-3.5-turbo perform well on the fact-checking rail and obtain an overall accuracy of 80 percent.

Evaluating the hallucination rail is difficult without employing subjective manual annotation. To overcome this issue and be able to automatically quantify its performance, a list of 20 questions based on a false premise is compiled, questions that do not have a right answer. Any generation from the language model, apart from deflection, is considered a failure. The benefit of employing the hallucination rail as a fallback mechanism is then quantified. For text-davinci-003, the LLM is unable to deflect prompts that are unanswerable and using the hallucination rail helps intercept 70 percent of these prompts. gpt-3.5-turbo performs much better, deflecting unanswerable prompts or marking that its response could be incorrect in 65 percent of the cases. Even in this case, employing the hallucination rail boosts performance up to 95 percent.

Conclusions and Limitations

NeMo Guardrails is a toolkit that allows developers to build controllable and safe LLM-based applications by implementing programmable rails. These rails are expressed using Colang and can also be implemented as custom actions if they require complex logic. Using CoT prompting and a dialogue manager that can interpret Colang code, the Guardrails runtime acts like a proxy between the application and the LLM enforcing the user-defined rails.

Building controllable and safe LLM-powered applications, in general, and dialogue systems, in particular, is a difficult task. The approach employed by NeMo Guardrails of using developer-defined programmable rails, implemented with prompting and the Colang interpreter, is not a perfect solution. Therefore, whenever possible, the toolkit should not be used as a stand-alone solution, especially for safety-specific rails. Programmable rails complement embedded rails and these two solutions should be used together for building safe LLM applications. The vision of the project is to also provide, in the future, more powerful customized models for some of the execution rails that should supplement the current pure prompting methods. Results show that adding the moderation rails to existing safety rails embedded in powerful LLMs, such as ChatGPT, provides better protection against jailbreak attacks.

In the context of controllable and task-oriented dialogue agents, it is difficult to develop customized models for all possible tasks and topical rails. Therefore, in this context, NeMo Guardrails is a viable solution for building LLM-powered task-oriented agents without extra mechanisms. However, even for topical rails and task-oriented agents, plans include releasing p-tuned models that achieve better performance for some of the tasks, such as for canonical form generation.

The three-step CoT prompting approach used by the Guardrails runtime incurs extra costs and extra latency. As these calls are sequentially chained, meaning the generation of the next steps in the second phase depends on the user canonical form generated in the first stage, the calls cannot be batched. In the current implementation, the latency and costs required are about 3 times the latency and cost of a normal call to generate the bot message without using Guardrails. The system is currently investigating if in some cases a single call could be used to generate all three steps: user canonical form, next steps in the flow, and bot message. Using a more complex prompt and few-shot in-context learning also generates slightly extra latency and a larger cost compared to a normal bot message generation for a vanilla conversation. Developers can decide to use a simpler prompt if needed. However, developers should be provided with various options for their needs. Some might be willing to pay the extra costs for having safer and controllable LLM-powered dialogue agents. Moreover, GPU inference costs will decrease and smaller models can also achieve good performance for some or all NeMo Guardrails tasks. As presented in the research, falcon-7b-instruct already achieves very good performance for topical rails. Similar positive performance has been seen from other recent models, like Llama 2, 7B and 13B chat variants.

Broader Impact

As a toolkit to enforce programmable rails for LLM applications, including dialogue systems, NeMo Guardrails should provide benefits to developers and researchers. Programmable rails supplement embedded rails, either general using RLHF or user-defined using p-tuned customized models. For example, using the fact-checking rail developers can easily build an enhanced retrieval-based LLM application and it also allows them to assess the performance of various models as programmable rails are model-agnostic. The same is true for building LLM-based task-oriented agents that should follow complex dialogue flows.

At the same time, before putting a Guardrails application into production, the implemented programmable rails should be thoroughly tested, especially safety related rails. The toolkit provides a set of evaluation tools for testing the performance both for topical and execution rails. Additional details for the toolkit can be found in documentation, including simple installation steps for running the toolkit with example Guardrails applications that are shared on Github.
