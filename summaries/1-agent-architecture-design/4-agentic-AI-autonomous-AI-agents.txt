Agentic AI Systems for Financial Services: Modeling and Model Risk Management
Traditional large language models operate in isolation, responding to single prompts without the capacity for structured planning, autonomous action, or adaptive memory across interactions. Agentic AI represents a fundamental evolution beyond this limitation—systems that can autonomously perceive goals, generate multi-step plans, execute actions by interfacing with external tools, adapt based on environmental feedback, and reason over time through continuous cycles of goal identification, planning, action execution, memory updates, and feedback reflection. This transformation addresses critical enterprise needs that static LLMs simply cannot meet: organizations demand true autonomy where AI systems can proactively make decisions and execute actions without constant human intervention, real-world environments are dynamic with constantly changing data and conditions requiring real-time adaptation, and safety and compliance are non-negotiable in enterprise settings where autonomous agents must operate under strict guardrails to ensure security, regulatory compliance, and ethical behavior.
The architecture of agentic systems rests on tightly integrated specialized components working in concert. At the foundation sits the agent core, containing information about the underlying NLP engine (like GPT models), the agent's goals, available tools, memory systems, and assigned persona. The memory module maintains both short-term memory tracking immediate context and actions, and long-term memory storing information across multiple sessions to enable personalized interaction. Tools serve as the agent's hands in the real world—external systems, APIs, specialized functions like retrieval-augmented generation for extracting contextually relevant information, web browsing capabilities, third-party integrations, and code execution environments. The planning module orchestrates decision-making by breaking complex tasks into manageable steps through task decomposition, while reflection mechanisms like ReAct, Reflexion, Chain of Thought, and Graph of Thought enable agents to evaluate their own actions, consider possible outcomes, and refine execution plans for greater accuracy and efficiency.
Within the CrewAI framework specifically, agents are defined by role-playing (specific identities that influence interactions), focus (concentration on assigned tasks without distraction), cooperation (collaborative information sharing and task delegation among agents), guardrails (safety measures preventing hallucinations and ensuring ethical operation), and memory systems encompassing short-term, long-term, entity, and contextual memory. Guardrails operate at multiple levels: low temperature parameters restrict LLM text generation to high-probability tokens for deterministic outputs, the separation between agent personas and task specifications prevents mission drift, and custom tools manage code execution outputs and log all actions including errors. The temperature parameter fundamentally controls token selection probability—at temperature 1, the model has creative freedom across its entire vocabulary (128,256 tokens for Llama3, 130,000 for Deepseek-R1, 100,256 for GPT-3.5 Turbo), while temperature 0 produces deterministic outputs by selecting only the highest softmax scores.
Collaboration strategies define how agents interact to accomplish shared objectives. Horizontal collaboration features agents with distinct roles and no hierarchical advantage, working at the same level—agents with similar goals collaborate while those with opposing goals negotiate or debate to reach collective decisions. Hierarchical collaboration introduces a leader agent that guides follower agents in executing instructions, creating vertical structure. Hybrid or nested collaboration combines both horizontal and vertical elements, while dynamic structures allow agent configurations to evolve adaptively in response to external factors or changing conditions. Effective collaboration in CrewAI specifically leverages information sharing so all agents communicate findings and stay informed, task assistance enabling agents to request help from specialists with relevant skills, and resource allocation for efficient distribution of computational resources to optimize task execution.
This research introduces two interconnected crews for financial services: a modeling crew and a model risk management crew, both operating with human-in-the-loop orchestration rather than pure autonomy. The critical insight driving this design is that agentic systems without human oversight can exhibit abnormal behavior due to LLM challenges—hallucinations, text chunking that loses overall context, and unpredictable errors especially when handling complex tasks in sensitive domains like finance. The human expert assumes the role of system orchestrator, overseeing task delegation, guiding agents in error correction when they fail autonomously, and providing suggestions to enhance outputs. A judge agent reviews actions performed by other agents and offers insights and recommendations to the human expert, enabling timely feedback and corrections while maintaining quality assurance appropriate for financial modeling's sensitivity and regulatory requirements.
The modeling crew comprises eight specialized agents working sequentially. The Data Extraction Agent functions as a data analyst, extracting data from external sources, splitting into training and testing sets to prevent data leakage, and subsampling training data for hyperparameter tuning. The EDA Agent performs exploratory data analysis identifying missing values, class imbalances, categorical variables, outliers, data distributions, skewness, and correlations. The Feature Engineering Agent creates preprocessing pipelines handling missing values through KNN imputation, feature normalization, ordinal encoding for categorical variables, and custom imputation—all based on data characteristics discovered during EDA. The Meta-Tuning Agent performs model selection and hyperparameter optimization using grid search on the subsample, powered by understanding different model strengths and weaknesses. The Model Training Agent trains the selected model with optimal hyperparameters on the full training set. The Model Evaluation Agent assesses performance using accuracy, F1-score, recall, precision, and AUC metrics on the test set. The Judge Agent examines how well coworkers performed using DeepSeek-R1's reasoning capabilities, and the Documentation Writer Agent compiles comprehensive technical documentation of all tasks using outputs from the human-in-the-loop module.
Memory and information flow operate through a memory stream object storing task delegations in natural language, execution timestamps, and information needed by collaborating agents. The core attribute is storing interconnected interactions from different agents—when the EDA Agent analyzes data characteristics, this information flows through the context parameter inherited from the Task module, allowing downstream agents like Feature Engineering to access insights about missing values, correlations, and distributions when building their preprocessing pipelines. This knowledge transfer through the human-in-the-loop module ensures each agent understands previous actions in relation to current tasks without requiring explicit re-communication.
The model risk management crew serves as a safeguard ensuring the modeling crew operates as intended while upholding regulatory rules, business objectives, and modeling standards. The Documentation Compliance Checker Agent verifies documentation against organizational modeling guides using Cache-Augmented Generation to compare steps and tasks with prescribed procedures. The Model Replication Agent independently replicates the model in a separate environment using exact hyperparameters to validate reproducibility and verify performance metrics align. The Conceptual Soundness Agent validates the model's conceptual framework, examining feature importance, interpretability, target objectives, data characteristics, and constraints. The Outcome Analyzer Agent tests robustness by perturbing test data to create adversarial inputs—regenerating inputs through multiplication or addition of fixed/randomized values to simulate extreme conditions, distribution shifts, and outlier inputs. The Judge and Documentation Writer Agents mirror their modeling crew counterparts but focus specifically on risk management assessment.
Experimental validation across three financial use cases demonstrates practical effectiveness. For credit card fraud detection with 284,807 transactions (99.83% non-fraudulent, significant class imbalance), the agentic system selected CatBoost achieving 99.9% accuracy, 97.6% precision, 81.6% recall, and 88.9% F1-score—outperforming H2O AutoML's best XGBoost model particularly on recall and F1-score, critical for fraud detection where capturing true positives matters most. MRM stress testing revealed vulnerability to shifted inputs (accuracy dropped to 99.8%, F1 to 65%, precision to 54.1%) but maintained robust performance on adversarial/outlier inputs (99.9% accuracy, 86.7% F1, 91% precision). For credit card approval prediction, the system achieved 99.9% across all metrics using CatBoost, matching both the Kaggle benchmark and AutoML's GLM model. MRM testing showed minimal decline for shifted inputs (99.8% accuracy and precision) with no significant degradation on adversarial inputs, suggesting well-represented data and strong pattern capture. For portfolio credit risk modeling with 32,581 loans, XGBoost achieved 93.4% accuracy, 97.2% precision, 72.4% recall, and 82.9% F1-score—slightly below Kaggle's CatBoost (93.72% accuracy) but comparable to AutoML's DRF (92.9% accuracy). However, MRM analysis revealed significant vulnerability to shifted inputs (79.1% accuracy, 60.5% F1, 52.2% precision) due to substantial distribution shifts across the predominantly numeric features, though adversarial input performance remained strong (92.8% accuracy, 81.4% F1, 96% precision).
The broader context reveals why this human-in-the-loop approach matters for financial services. LLM-based multi-agent systems carry inherent risks: they can amplify biases affecting marginalized groups, exhibit error propagation and hallucinations, lack transparency making them appear as "black boxes" where decision-making is opaque, induce preferences or toxic degeneration, and make it difficult to assess how agents reach conclusions. Safety concerns range from unproven theories and questionable data sources in training to retrieval-augmented systems pulling false information from internet sources, potentially producing discriminatory behavior, harmful results, or offensive content. The human-in-the-loop approach provides oversight, feedback, and intervention to prevent harmful actions. Combined with guardrails—rules ensuring operational safety through layered protection models, system prompts, RAG architectures, and techniques minimizing bias while protecting privacy—the system reduces likelihood of dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. This triadic framework involving human regulation, agent alignment, and environmental feedback creates the trust and transparency foundation necessary for deploying AI in heavily regulated financial services.
The research demonstrates that agentic systems can effectively handle complex financial modeling workflows—conducting exploratory analysis, engineering features, selecting and tuning models, training, and evaluating—while model risk management crews validate compliance, replicate results, assess conceptual soundness, and test robustness under extreme conditions. The modular architecture allows specialization where each agent develops deep expertise in its domain, collaboration enables knowledge sharing and task assistance across the workflow, and human orchestration ensures quality, compliance, and correction when autonomous agents encounter limitations. This represents a practical pathway for financial institutions to leverage LLM capabilities for sophisticated analytical tasks while maintaining the rigorous oversight and risk management that regulatory environments demand.

KEY TERMS AND DEFINITIONS

Agent Core: The foundational component of an agentic system containing information about the underlying NLP engine (like GPT models), the agent's goals, available tools, memory systems, and assigned persona. This core defines what the agent is and what it can do.

CrewAI Framework: A framework for building multi-agent systems where agents are defined by role-playing (specific identities that influence interactions), focus (concentration on assigned tasks), cooperation (collaborative information sharing), guardrails (safety measures), and memory systems. CrewAI enables structured collaboration between specialized agents.

Temperature Parameter: A critical control mechanism for LLM text generation that fundamentally controls token selection probability. At temperature 1, the model has creative freedom across its entire vocabulary, while temperature 0 produces deterministic outputs by selecting only the highest softmax scores. Lower temperatures are used in financial services for more predictable, reliable outputs.

Horizontal Collaboration: A collaboration strategy where agents with distinct roles work at the same level with no hierarchical advantage. Agents with similar goals collaborate, while those with opposing goals negotiate or debate to reach collective decisions.

Hierarchical Collaboration: A collaboration strategy that introduces a leader agent that guides follower agents in executing instructions, creating vertical structure. This provides oversight and coordination for complex workflows.

Human-in-the-Loop Orchestration: A critical design pattern where human experts assume the role of system orchestrator, overseeing task delegation, guiding agents in error correction, and providing suggestions to enhance outputs. This approach is essential for sensitive domains like finance where autonomous agents need oversight.

Modeling Crew: A team of eight specialized agents working sequentially to handle financial modeling workflows: Data Extraction Agent, EDA Agent, Feature Engineering Agent, Meta-Tuning Agent, Model Training Agent, Model Evaluation Agent, Judge Agent, and Documentation Writer Agent.

Model Risk Management (MRM) Crew: A safeguard team that ensures the modeling crew operates as intended while upholding regulatory rules, business objectives, and modeling standards. It includes agents for compliance checking, model replication, conceptual soundness validation, and outcome analysis.

Memory Stream Object: A mechanism for storing task delegations in natural language, execution timestamps, and information needed by collaborating agents. This enables knowledge transfer between agents without requiring explicit re-communication.

Cache-Augmented Generation: A technique used by the Documentation Compliance Checker Agent to compare steps and tasks with prescribed procedures by augmenting the generation process with cached organizational modeling guides and standards.

Adversarial Input Testing: A robustness testing technique performed by the Outcome Analyzer Agent that perturbs test data to create adversarial inputs—regenerating inputs through multiplication or addition of fixed/randomized values to simulate extreme conditions, distribution shifts, and outlier inputs.

Concept Drift: The phenomenon where the statistical properties of the target variable change over time, causing model performance to degrade. In fraud detection, this occurs when attacker behavior evolves faster than model retraining cycles.

REVIEW QUESTIONS

1. Why is human-in-the-loop orchestration described as "critical" for financial services applications? What specific risks does it address that pure autonomy cannot handle, and how does the judge agent support this oversight?

2. The article describes four collaboration strategies: horizontal, hierarchical, hybrid, and dynamic. Compare and contrast horizontal and hierarchical collaboration, explaining when each would be most appropriate for financial modeling tasks.

3. How does the temperature parameter control the balance between creativity and determinism in LLM outputs? Why is a lower temperature (closer to 0) particularly important for financial services applications, and what risks does higher temperature introduce?

4. The modeling crew comprises eight specialized agents working sequentially. Trace how information flows from the Data Extraction Agent through to the Documentation Writer Agent, explaining how each agent builds on the work of previous agents.

5. What is the relationship between the modeling crew and the model risk management crew? How does the MRM crew provide independent validation while still being "interconnected" with the modeling crew?

6. The Memory Stream Object enables "knowledge transfer through the context parameter." Explain how this mechanism allows downstream agents (like Feature Engineering) to access insights from upstream agents (like EDA) without explicit re-communication.

7. Experimental results show that models can be vulnerable to "shifted inputs" but robust to "adversarial inputs." What does this distinction mean, and why might a model perform differently on these two types of test cases?

8. The article mentions that LLM-based multi-agent systems can "amplify biases affecting marginalized groups" and "exhibit error propagation." How does the human-in-the-loop approach combined with guardrails address these risks in financial services?

9. Consider the credit card fraud detection use case. Why was recall and F1-score particularly important metrics for this application, and how did the agentic system's performance on these metrics compare to traditional AutoML approaches?

10. The triadic framework involves "human regulation, agent alignment, and environmental feedback." Explain how each component contributes to creating the "trust and transparency foundation" necessary for deploying AI in heavily regulated financial services.