[Music] Hello everyone. In this video, we'll  be building a multimodal AI agent with RAG  
0:15
capabilities, leveraging NVIDIA's GPU-accelerated  models and open-source technologies. Let's start  
0:24
with a quick demo of our chatbot in action.  As you can see, our chatbot can quickly  
0:30
process and respond to the question about  the data provided. So, in this tutorial,  
0:35
we'll cover integrating a pipeline that accepts  multimodal kinds of data that includes text and  
0:42
vision. As part of the process, we'll cover how  to set up a vector database, generate embeddings,  
0:48
and how to use GPU-accelerated models. Now, let's  break down how we build this. Before we dive into  
0:56
the specifics of our chatbot, it's important  to understand the ecosystem we are working  
1:01
with. NVIDIA provides a full-stack ecosystem for  large language models that integrates seamlessly  
1:08
with popular open-source tools. This ecosystem  offers flexibility and performance, allowing  
1:14
developers to switch between different components  as needed. NVIDIA not only provides proprietary  
1:21
tools but also contributes significantly to  the open-source community. Tools like NeMo  
1:27
for building and training large language models,  and Triton for INF serving, are prime examples of  
1:32
NVIDIA's contribution to open source. These tools  integrate well with the broader AI ecosystem,  
1:39
providing developers with powerful GPU-accelerated  options for building LLM applications. NVIDIA  
1:46
NeMo goes beyond just a training framework; it  includes open-source components that enhance  
1:52
the capabilities for developers. Two notable  open-source components are NeMo Data Curator,  
1:58
that simplifies the complex process of data  curation. It helps extract, deduplicate,  
2:04
and filter information from large amounts of  unstructured data, ensuring high-quality, relevant  
2:10
data sets for training. The second component  is NeMo Guard R. This component implements  
2:15
safety measures and controls for model output.  It allows for developers to add a guard that  
2:21
prevents inappropriate responses, enhancing the  reliability and safety of AI applications. You can  
2:30
learn more about this at github.com/NVIDIA/NeMo.  Let's quickly understand what RAG, or Retrieval  
2:39
Augmented Generation, is. So, RAG enhances large  language models by giving them access to external  
2:45
up-to-date knowledge. This process allows our  chatbot to provide responses that are both general  
2:52
and specific to our patent data, combining  retrieval and generation for more accurate  
2:58
and contextual answers. Let's explore how we are  extending RAG to handle various types of data, or  
3:07
what we call a multimodal RAG pipeline. Now, there  are various ways to handle multimodal data, but in  
3:14
this case, we are grounding all the modality into  a single one, which is text. For this specific  
3:20
application, we have introduced a specialized  Vision Language Model to process visual data. So,  
3:28
in this case, one V is handling general images,  while the other Vision Language Model is  
3:34
specifically tuned for charts and plots. Once the  conversion of information is done, the rest of the  
3:42
pipeline remains the same as the traditional  RAG pipeline. For our implementation of this  
3:49
application, we are using cutting-edge tools  from NVIDIA and the open-source community. So,  
3:55
let's break down the components of this specific  pipeline. To build our knowledge base, we first  
4:02
have to convert all the documents into text form,  and to do that, we'll use Vision Language Models,  
4:10
as shared before. For general image  understanding, we'll be using Neva 22B model,  
4:17
which is NVIDIA's fine-tuned variant of LAVA. For  charts and plots, we are employing DEOT by Google,  
4:25
which is specialized in understanding graphical  data, including charts and plots. For this  
4:32
specific example, we'll have some mix of  documents, which is PDF and images that  
4:38
contain some of the blogs and slides from NVIDIA.  So, this data fuels our chatbot's knowledge base.  
4:45
For vector storage and similarity search, we'll  use Milvus, a GPU-accelerated vector database  
4:51
that can handle large-scale similarity searches  with impressive speed. So, using GPU indexing and  
4:57
querying requires numerous queries, vectors, or  exceptionally high request pressure for maximum  
5:03
throughput and reduced latency compared to CPU  indexing. For embedding, we'll use NV Embed,  
5:12
which leverages NVIDIA's GPU acceleration to  quickly transform text into high-dimensional  
5:17
vectors. The GPU acceleration in both NV  Embed and Milvus significantly enhances the  
5:23
performance of our chatbot, allowing for faster  indexing and querying of our knowledge base.  
5:30
For the actual question answering, we'll use  a large language model, specifically LLaMA  
5:35
3 from Meta, the 70B instruct variant, which  will be accessed through NVIDIA's NIM API. So,  
5:43
this gives us the power of state-of-the-art  language models, which is GPU-optimized access  
5:49
through an API. Finally, we'll use LLaMA  Index to orchestrate the entire process,  
5:57
from query processing to retrieving relevant  information and generating responses. We will  
6:04
also use Streamlit to create a user-friendly  interface for our chatbot, where the user can  
6:11
directly upload or just provide the directory  path of their documents, and then simply query  
6:17
or ask a question based on those documents. Now  we have learned what the pipeline is about. Now  
6:24
let's dive into the code. Before we dive into the  individual script, let's take a quick look at our  
6:30
project structure. Our project consists of three  main Python scripts. The first one is app.py,  
6:37
which is the main application file that sets up  the Streamlit interface and manages the overall  
6:42
flow of our multimodal RAG system. The second  is document_processors.py, which is responsible  
6:49
for processing various document types like PDFs,  PowerPoint presentations, and images. Then comes  
6:57
the utils.py, which contains utility functions  for image processing, API interaction, and text  
7:04
handling. In addition to these Python files,  our project also includes several directories,  
7:12
such as Vector Store, which stores all the vector  database and related files. In Vector Store,  
7:19
we have image references and PPT references, which  store image-extracted or converted PowerPoint  
7:28
slides and its related files. This structure  allows us to efficiently manage different  
7:34
types of data and maintain a clean separation of  concerns in our code. Before we dive deep into our  
7:41
RAG Q&A chat application code, let's generate an  API key for NIMS by going on to build.nvidia.com,  
7:48
and we'll select LLaMA 3 as our LLM model for the  application. Here you get various options to try  
7:55
out the model or just boil aate code to get this  model on NIM optimized NIM onto your application,  
8:04
but here we are trying to generate an API key, so  just click generate, and there you have it. Just  
8:11
copy this API key, and you can use it wherever  you are setting the environment variable. But in  
8:17
my case, I'll set this on my terminal, assign that  API key to a variable named NVIDIA_API_UNCORE_KEY  
8:28
as an environment variable, and you can do that  with this command and press enter. Let's start  
8:35
with utils.py, which contains various utility  functions for our multimodal project. Here we  
8:41
are importing necessary libraries and setting  up our NVIDIA API key. This is an alternative  
8:47
way from what we did in the terminal,  but you can choose either one of those,  
8:53
whatever is better for you. And then now we load  an image processing function, which can handle  
9:00
image conversion to base 64 format, which is  necessary for sending images to NVIDIA NIM APIs.  
9:08
And then we use an image analysis function, which  helps us identify and process the graphs or charts  
9:16
in our images. We use NVIDIA's NIM APIs, where  the model are hosted, to generate a description of  
9:24
these visual elements. And now we are using these  functions to interact with NIM API to describe  
9:32
images and process graphs, which are crucial for  our multimodal understanding capabilities. Next,  
9:40
we'll create some text processing functions to  process text from PDF, allowing us to extract  
9:47
context around images and tables and group text  blocks efficiently. And now we'll do some file  
9:54
handling, which handles saving uploaded files,  which is important for processing user-provided  
10:01
documents. So that covers the main utilities in  our utils.py. These functions form the backbone  
10:08
of our image and text processing capabilities.  Now let's move on to the document processor.py.  
10:14
Now let's go through the document_processor.py  file, which contains functions for processing  
10:20
various types of documents. So, first, we  start by importing necessary utilities,  
10:26
and notice that we also importing functions that  we just defined in the utils.py. Now we'll create  
10:33
a function that is crucial for extracting content  from the PDF files, including text, images,  
10:41
and tables. Now we write a function for handling  the extraction of tables and images from the  
10:49
PDF files, including processing them and storing  them. Now let's work on the PowerPoint processing,  
10:57
and here we are writing four different functions  that work together to process PowerPoint files,  
11:03
converting them into PDFs, and then to images,  while also extracting text and notes. And now  
11:10
write the main function, which handles the  loading and processing of various file types,  
11:16
whether they are individual files or entire  directories. So that covers the main component  
11:23
of our document_processor.py file. Now let's move  on to the main script, which is app.py, which  
11:30
includes all the GUI and function calling for all  these scripts that we have just written. Now let's  
11:36
move on to app.py, which is the main application  file for our project. We start by importing  
11:44
necessary libraries and set up our Streamlit page.  Notice how we are using LLaMA Index components  
11:50
and our custom modules. Then we set up our  embedding models and language models using  
11:56
NVIDIA's NIM integration with LLaMA Index. And  then we create the index, which uses the vector  
12:06
database using Milvus and stores all our document  embeddings. Now this is the main application,  
12:14
where we set up our Streamlit interface.  It provides options to upload or directory  
12:21
processing and sets up the chat interface. And  now this section handles the chat interface,  
12:29
processes user queries, and displays responses.  So that covers the main component of our app.py  
12:36
files and all the other Python scripts. Now let's  try running this code on Terminal. Before we run  
12:44
the code, let's create a conda environment and  install all the dependencies that we need for this  
12:50
project. First, we'll create a conda environment  using the conda create command. I'll name this  
12:59
environment as GPU_Rag, and I'll have Python  3.0, so it will create the environment. Now let's  
13:14
activate the environment that we just created by  using conda activate and the environment name. Now  
13:21
we are into the environment. Let's install all  the required dependencies by using pip install  
13:29
method and passing in the requirements.txt  file, which includes all the dependencies. Now  
13:40
dependencies are installed. Let's try running this  app using Streamlit. To do that, you mainly use  
13:46
Streamlit run and app.py, so it should generate  a port number where you can use to look at the  
13:55
web app. So here it has given us the local URL,  Network URL, and external URL as well. So, it's  
14:05
8501. So let's try to go into the browser and see  what's there. So this is how the UI looks like,  
14:13
where you have an option to upload files or  enter a directory path, where it will process  
14:19
all the documents present in that directory. So  in this case, I'll upload a mix of documents,  
14:26
which is a PDF of a blog from NVIDIA developer,  which includes images as well as graphs doing  
14:34
various performance comparisons or the speed up,  and then I'll also add an image or a slide about  
14:41
TensorRT. So let's try importing those files. So  once the documents are uploaded to the server,  
14:52
I can process these files, where it is generating  embeddings and creating a vector database,  
14:59
where you can query or ask questions about  these documents that you just uploaded. So it  
15:05
may take some time or a while, based on the size  of the documents. Now that the index is created,  
15:13
let's try asking a question based on the document  we just uploaded. So the first question I'll ask  
15:19
is like, what is the variant about LLaMA that  is the fastest? So based on the chart in this  
15:29
blog, it answered correctly, which is 7B Q4.  Now let's ask, how does CUDA graph lead to  
15:39
substantial performance? So again, it uses the  context. And then let's wrap it up by asking,  
15:49
who's the author, which is Alan G, based on  the doc we just uploaded. So there you have it.  
15:57
Users can now upload documents of various input  types, including text and vision, ask questions,  
16:04
and receive answers based on the content of those  documents in real time. So in this tutorial,  
16:10
we have covered how to build a multimodal  AI-based RAG application for enhanced  
16:16
understanding using GPU-accelerated  components from NVIDIA and open-source  
16:21
tools. Feel free to experiment with the  provided scripts and explore further. For  
16:26
more information and resources, check  the links in the description below.