Overview
NeMo RL is an open-source post-training library within the NeMo Framework, designed to streamline and scale reinforcement learning methods for multimodal models (LLMs, VLMs, etc.). Designed for flexibility, reproducibility, and scale, NeMo RL enables both small-scale experiments and massive multi-GPU, multi-node deployments for fast experimentation in research and production environments.

What You Can Expect
Flexibility with a modular design that allows easy integration and customization.

Efficient resource management using Ray, enabling scalable and flexible deployment across different hardware configurations.

Hackable with native PyTorch-only paths for quick research prototypes.

High performance with Megatron Core, supporting various parallelism techniques for large models and large context lengths.

Seamless integration with Hugging Face for ease of use, allowing users to leverage a wide range of pre-trained models and tools.

Comprehensive documentation that is both detailed and user-friendly, with practical examples.

For more details on the architecture and design philosophy, see the design documents.
Quick Start
Use this quick start to get going with either the native PyTorch DTensor or Megatron Core training backends.

Note

Both training backends are independent — you can install and use either one on its own.

For more examples and setup details, continue to the Prerequisites section.

Quick Start Options
Native PyTorch (DTensor)

Megatron Core

Clone and create the environment

git clone git@github.com:NVIDIA-NeMo/RL.git nemo-rl
cd nemo-rl
git submodule update --init --recursive
uv venv
Note

If you previously ran without checking out the submodules, you may need to rebuild virtual environments by setting NRL_FORCE_REBUILD_VENVS=true. See Tips and Tricks.

Native PyTorch (DTensor)

Megatron Core

Run GRPO (DTensor)

Run GRPO (Megatron)

# DTensor
uv run python examples/run_grpo_math.py
# Megatron
uv run examples/run_grpo_math.py \
  --config examples/configs/grpo_math_1B_megatron.yaml
  Features and Roadmap
Available now | Coming in v0.4

Coming in v0.4
Megatron Inference - Megatron Inference for fast Day-0 support for new Megatron models (avoid weight conversion)

Async RL - Support for asynchronous rollouts and replay buffers for off-policy training, and enable a fully asynchronous GRPO

Vision Language Models (VLM) - Support SFT and GRPO on VLMs through the DTensor path

Improved Native Performance - Improve training time for native PyTorch models

Improved Large MoE Performance - Improve Megatron Core training performance and generation performance

End-to-End FP8 Low-Precision Training - Support for Megatron Core FP8 training and FP8 vLLM generation

Megatron Bridge Integration - Integrate Megatron Bridge to enable training features from Megatron Core

NeMo Automodel Integration - Integrate NeMo Automodel to power the DTensor path

New Models - gpt-oss

Expand Algorithms - DAPO, GSPO, On-policy Distillation

GB200 - Add container support for GB200

Available Now
Distributed Training - Ray-based infrastructure

Environment Support and Isolation - Support for multi-environment training and dependency isolation between components

Worker Isolation - Process isolation between RL Actors (no worries about global state)

Learning Algorithms - GRPO/GSPO, SFT, and DPO

Multi-Turn RL - Multi-turn generation and training for RL with tool use, games, etc

Advanced Parallelism with DTensor - PyTorch FSDP2, TP, CP, and SP for efficient training

Larger Model Support with Longer Sequences - Performant parallelisms with Megatron Core (TP/PP/CP/SP/EP/FSDP)

MoE Models - Support for DeepSeekV3 and Qwen-3 MoE models (Megatron)

Sequence Packing - Sequence packing in both DTensor and Megatron Core for huge training performance gains

Fast Generation - vLLM backend for optimized inference

Hugging Face Integration - Works with 1B–70B models (Qwen, Llama)
Training and Generation Backends
Training Backends
NeMo RL supports multiple training backends to accommodate different model sizes and hardware configurations:

PyTorch - This leverages NeMo AutoModel to provide accelerated PyTorch training with improved memory efficiency (PyTorch-native TP, SP, PP, CP, and FSDP2)

Megatron - NVIDIA’s high-performance training framework for scaling to large models with 6D parallelisms

The training backend is automatically determined based on your YAML configuration settings. For detailed information on backend selection, configuration, and examples, see the Training Backends documentation.

Generation Backends
NeMo RL supports multiple generation/rollout backends to accommodate different model sizes and hardware configurations:

vLLM - A high-throughput and memory-efficient popular inference and serving engine

Megatron - A high-performance Megatron-native inference backend which eliminates weight conversion between training and inference

For detailed information on backend selection, configuration, and examples, see the Generation Backends documentation.

Algorithms
NeMo RL supports multiple training algorithms for post-training large language models.

Support Matrix
Algorithms

Single Node

Multi-node

GRPO

GRPO Single Node

GRPO Multi-node: GRPO Qwen2.5-32B, GRPO Multi-Turn

DAPO (dapo.md)

similar to GRPO example

similar to GRPO example

DAPO

DAPO Single Node

DAPO Multi-node

On-policy Distillation

Distillation Single Node

Distillation Multi-node

Supervised Fine-Tuning (SFT)

SFT Single Node

SFT Multi-node

DPO

DPO Single Node

DPO Multi-node

RM

RM Single Node

RM Multi-node

On-policy distillation is also supported in the PyTorch DTensor path.



Evaluation
We provide evaluation tools to assess model capabilities.

Convert Model Format (Optional)
If you have trained a model and saved the checkpoint in the PyTorch DCP format, you first need to convert it to the Hugging Face format before running evaluation:

# Example for a GRPO checkpoint at step 170
uv run python examples/converters/convert_dcp_to_hf.py \
    --config results/grpo/step_170/config.yaml \
    --dcp-ckpt-path results/grpo/step_170/policy/weights/ \
    --hf-ckpt-path results/grpo/hf
If you have a model saved in Megatron format, you can use the following command to convert it to Hugging Face format prior to running evaluation. This script requires Megatron Core, so make sure you launch with the mcore extra:

# Example for a GRPO checkpoint at step 170
uv run --extra mcore python examples/converters/convert_megatron_to_hf.py \
    --config results/grpo/step_170/config.yaml \
    --megatron-ckpt-path results/grpo/step_170/policy/weights/iter_0000000 \
    --hf-ckpt-path results/grpo/hf
Note

Adjust the paths according to your training output directory structure.

For an in-depth explanation of checkpointing, refer to the Checkpointing documentation.

Run Evaluation
Run the evaluation script with the converted model:

uv run python examples/run_eval.py generation.model_name=$PWD/results/grpo/hf
Run the evaluation script with custom settings:

# Example: Evaluation of DeepScaleR-1.5B-Preview on MATH-500 using 8 GPUs
#          Pass@1 accuracy averaged over 16 samples for each problem
uv run python examples/run_eval.py \
    --config examples/configs/evals/math_eval.yaml \
    generation.model_name=agentica-org/DeepScaleR-1.5B-Preview \
    generation.temperature=0.6 \
    generation.top_p=0.95 \
    generation.vllm_cfg.max_model_len=32768 \
    data.dataset_name=math500 \
    eval.num_tests_per_prompt=16 \
    cluster.gpus_per_node=8
Note

Evaluation results may vary slightly due to various factors, such as sampling parameters, random seed, inference engine version, and inference engine settings.

Refer to examples/configs/evals/eval.yaml for a full list of parameters that can be overridden. For an in-depth explanation of evaluation, refer to the Evaluation documentation.